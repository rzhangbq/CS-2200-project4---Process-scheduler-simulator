CS 2200 Fall 2022
Project 4

Name: Rushan Zhang
GT Username: 903849603

Problem 1B (FCFS Scheduler)
----------
/* Fix me */
1. total execution time:
number of CPUs  |  Execution time
1               |  67.9 s
2               |  39.9 s
4               |  37.0 s

2. there is not a linear relationship between the number of CPUs and the total execution time.
3. the reason is:
    1) There is only one I/O queue with one I/O handler. As the number of CPUs increases, more processes are stuck on the I/O queue, leaving the CPU idle. (i.e. even though time spent in ready state decreases, time spent in waiting state increases)
    2) There are not so many processes running concurrently, 

Problem 2B (Round-Robin)
----------
/* Fix me */
1. total execution time:
number of CPUs  |RR time slice  |Execution time
1               |800ms          |67.9 s
1               |600ms          |67.9 s
1               |400ms          |67.9 s
1               |200ms          |67.9 s
---------------------------------------------------
2               |800ms          |40.1 s
2               |600ms          |40.7 s
2               |400ms          |40.9 s
2               |200ms          |40.9 s
---------------------------------------------------
4               |800ms          |37.3 s
4               |600ms          |37.6 s
4               |400ms          |37.6 s
4               |200ms          |37.0 s

2. relationship between waiting time and time slice
From my execution, there is no significant relationship between total execution time and RR time slice. However, according to the running results on Gradescope, it seems that the average execution time decreases slightly as time slice decreases.

3. why not the shortest time slice a good idea?
In our simulation, it seems that we did not take into account the time spent on context switch. However, in a real OS, context switch could be expensive. With shorter time slice, the number of context switch increases dramatically. As a result, with a shorter time slice, the time spent on context switch increases dramatically.

Problem 3B (Preemptive Priority)
----------
/* Fix me */
1. How can an OS mitigate starvation in a priority scheduler?
This can be mitigated by dynamically increasing the priority of waiting process as its waiting time increases.

Problem 4 (The Priority Inversion Problem)
---------
/* Fix me */
A possible solution is that we increase the priority of lower-priority processes when they are controlling resources requested by higher-priority processes to the same priority level.
After the low-priority processes give up the requested resources, they resume their previous lower priority.
    i.e. Z is using S. When X, a higher priority process requests S, we can promote Z to the same priority as X. As a result, Y, which has lower priority than X, has lower priority than Z temporarily, so Y cannot preempt Z.
        As soon as Z gives up S, it resumes its lower priority, so Y has higher priority than Z and Y can preempt Z. Since Z has given up S, X can continue to use S.

====notes====
After I think of the solution above, I searched on wikipedia: https://en.wikipedia.org/wiki/Priority_inversion
It seems that the solution I propose is called "priority inheritance".
There are other solutions. For example, we can disable preemption when entering a critical section (i.e. we can disable preemption of Z when Z has control of S). This is done by "priority ceiling protocol".
Other solution includes "random boosting", "avoid blocking", and "disabling all interrupts to protect critical sections".
